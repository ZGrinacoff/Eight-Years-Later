{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Resources/hate_crime.csv' does not exist: b'Resources/hate_crime.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e6893aded52a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhate_crime_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Resources/hate_crime.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhate_crime_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhate_crime_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhate_crime_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Resources/hate_crime.csv' does not exist: b'Resources/hate_crime.csv'"
     ]
    }
   ],
   "source": [
    "hate_crime_csv = pd.read_csv(\"Resources/hate_crime.csv\", low_memory=False)\n",
    "hate_crime_df = pd.DataFrame(hate_crime_csv)\n",
    "hate_crime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce DF to include only neccessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['INCIDENT_ID', 'DATA_YEAR', 'INCIDENT_DATE', 'PUB_AGENCY_NAME', 'AGENCY_TYPE_NAME',\n",
    "           'STATE_ABBR', 'STATE_NAME', 'POPULATION_GROUP_DESC', 'TOTAL_OFFENDER_COUNT',\n",
    "           'TOTAL_INDIVIDUAL_VICTIMS', 'LOCATION_NAME', 'BIAS_DESC', 'VICTIM_TYPES'\n",
    "          ]\n",
    "\n",
    "reduced_hate_crime_df = hate_crime_df.loc[:,  columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data: 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter DF for 2009 only.\n",
    "reduced_hate_crime_2009_df = reduced_hate_crime_df.loc[(reduced_hate_crime_df['DATA_YEAR'] == 2009)]\n",
    "reduced_hate_crime_2009_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new DF for 2009 from value counts. Total number of hate crimes committed per state.\n",
    "\n",
    "states2009 = reduced_hate_crime_2009_df['STATE_NAME'].value_counts(sort=True)\n",
    "state_hatecrime_2009_df = pd.DataFrame(states2009)\n",
    "state_hatecrime_2009_df = state_hatecrime_2009_df.reset_index()\n",
    "state_hatecrime_2009_df.columns = ['State', 'Hate Crimes Committed']\n",
    "state_hatecrime_2009_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census imports for 2012.\n",
    "\n",
    "from census import Census\n",
    "from us import states\n",
    "\n",
    "# Census API Key\n",
    "# Data not available for 2010 0r 2011.\n",
    "from config import api_key\n",
    "c = Census(api_key, year=2012)\n",
    "#c.acs5.tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Census Search to retrieve data on all states\n",
    "\n",
    "census_data = c.acs5.get((\"NAME\", \"B19013_001E\", \"B19301_001E\", \"B23025_005E\", \"B23025_002E\",\n",
    "                          \"B17001_002E\", \"B17001_003E\", \"B17001_017E\",\n",
    "                          \"B17001A_002E\", \"B17001B_002E\", \"B01003_001E\",\n",
    "                          \"B02001_002E\", \"B02001_003E\", \"B15003_017E\", \"B15003_022E\"), {'for': 'state:*'})\n",
    "\n",
    "census_df = pd.DataFrame(census_data)\n",
    "\n",
    "census_df = census_df.rename(columns={\"B19301_001E\": \"Per Capita Income\",\n",
    "                                      \"B19013_001E\": \"Median Household Income\",\n",
    "                                      \"B23025_005E\": \"Unemployment Count\",\n",
    "                                      \"B23025_002E\": \"Labor Force Size\",\n",
    "                                      \"B17001_002E\": \"Poverty Count\",\n",
    "                                      \"B17001_003E\": \"Poverty: Male\",\n",
    "                                      \"B17001_017E\": \"Poverty: Female\",\n",
    "                                      \"B17001A_002E\": \"Poverty: White\",\n",
    "                                      \"B17001B_002E\": \"Poverty: Black\",\n",
    "                                      \"B01003_001E\": \"Total Population\",\n",
    "                                      \"B02001_002E\": \"Population: White\",\n",
    "                                      \"B02001_003E\": \"Population: Black\",\n",
    "                                      \"B15003_017E\": \"Education: High School\",\n",
    "                                      \"B15003_022E\": \"Education: Bachelors\",\n",
    "                                      \"NAME\": \"State\", \"state\": \"State Number\"})\n",
    "\n",
    "# Calculate & add in Poverty Rate (Poverty Count / Population)\n",
    "census_df[\"Poverty Rate\"] = 100 * \\\n",
    "    census_df[\"Poverty Count\"].astype(\n",
    "        int) / census_df[\"Total Population\"].astype(int)\n",
    "\n",
    "# Calculate & add in Unemployment Rate (Unemployment Count / Labor Force)\n",
    "census_df[\"Unemployment Rate\"] = 100 * \\\n",
    "    census_df[\"Unemployment Count\"].astype(\n",
    "        int) / census_df[\"Labor Force Size\"].astype(int)\n",
    "\n",
    "# Calculate and add share of population with at least a high school diploma.\n",
    "census_df['Share of Population with HS Diploma'] = 1 - \\\n",
    "    census_df['Education: High School'].astype(\n",
    "        int) / census_df['Total Population'].astype(int)\n",
    "\n",
    "census_df = census_df[[\"State\", \"Total Population\", \"Median Household Income\", \"Per Capita Income\", \"Unemployment Rate\",\n",
    "                         \"Poverty Rate\", \"Share of Population with HS Diploma\"]]\n",
    "\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GINI index for 2009.\n",
    "gini2009_csv = pd.read_csv('Resources/GINI2009.csv')\n",
    "\n",
    "# Merge csv with census DF on State.\n",
    "census_df = pd.merge(census_df, gini2009_csv, how='left', on='State')\n",
    "\n",
    "# Merge Hate Crime DF with census DF.\n",
    "clean_hate_crime_2009_df = pd.merge(census_df, state_hatecrime_2009_df, how=\"left\", on=\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values for states w/ no data (reported hate crimes) with 0.\n",
    "# Hawaii is now the only stat w/ no data (reported hate crimes).\n",
    "values = {'Hate Crimes Committed': 0}\n",
    "clean_hate_crime_2009_df = clean_hate_crime_2009_df.fillna(value=values)\n",
    "#clean_hate_crime_2009_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Puerto Rico from DF.\n",
    "clean_hate_crime_2009_df = clean_hate_crime_2009_df.drop([51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2009 Hate Crime Rate per 100,000 total population.\n",
    "# https://oag.ca.gov/sites/all/files/agweb/pdfs/cjsc/prof10/formulas.pdf\n",
    "# Note:\t Calculating rates for geographies of less than 100,000 will generate an inflated rate\n",
    "# when compared to geographies with populations of 100,000 or more; therefore,\n",
    "# rates are not calculated for geographies with populations of less than 100,000.\n",
    "\n",
    "clean_hate_crime_2009_df['Hate Crime Rate'] = 100000 * \\\n",
    "    clean_hate_crime_2009_df['Hate Crimes Committed'].astype(\n",
    "        int) / clean_hate_crime_2009_df['Total Population'].astype(int)\n",
    "\n",
    "# Sort descending on Hate Crime Rate.\n",
    "clean_hate_crime_2009_df = clean_hate_crime_2009_df.sort_values(by='Hate Crime Rate', ascending=False)\n",
    "clean_hate_crime_2009_df = clean_hate_crime_2009_df.reset_index(drop=True)\n",
    "clean_hate_crime_2009_df\n",
    "\n",
    "clean_hate_crime_2009_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data: 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DF for 2017 only.\n",
    "reduced_hate_crime_2017_df = reduced_hate_crime_df.loc[(reduced_hate_crime_df['DATA_YEAR'] == 2017)]\n",
    "reduced_hate_crime_2017_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DF for 2017 from value counts. Total number of hate crimes committed per state.\n",
    "states2017 = reduced_hate_crime_2017_df['STATE_NAME'].value_counts(sort=True)\n",
    "state_hatecrime_2017_df = pd.DataFrame(states2017)\n",
    "state_hatecrime_2017_df = state_hatecrime_2017_df.reset_index()\n",
    "state_hatecrime_2017_df.columns = ['State', 'Hate Crimes Committed']\n",
    "state_hatecrime_2017_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census imports for 2016.\n",
    "\n",
    "c = Census(api_key, year=2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Census Search to retrieve data on all states\n",
    "\n",
    "census_data = c.acs5.get((\"NAME\", \"B19013_001E\", \"B19301_001E\", \"B23025_005E\", \"B23025_002E\",\n",
    "                          \"B17001_002E\", \"B17001_003E\", \"B17001_017E\",\n",
    "                          \"B17001A_002E\", \"B17001B_002E\", \"B01003_001E\",\n",
    "                          \"B02001_002E\", \"B02001_003E\", \"B15003_017E\", \"B15003_022E\"), {'for': 'state:*'})\n",
    "\n",
    "census_df = pd.DataFrame(census_data)\n",
    "\n",
    "census_df = census_df.rename(columns={\"B19301_001E\": \"Per Capita Income\",\n",
    "                                      \"B19013_001E\": \"Median Household Income\",\n",
    "                                      \"B23025_005E\": \"Unemployment Count\",\n",
    "                                      \"B23025_002E\": \"Labor Force Size\",\n",
    "                                      \"B17001_002E\": \"Poverty Count\",\n",
    "                                      \"B17001_003E\": \"Poverty: Male\",\n",
    "                                      \"B17001_017E\": \"Poverty: Female\",\n",
    "                                      \"B17001A_002E\": \"Poverty: White\",\n",
    "                                      \"B17001B_002E\": \"Poverty: Black\",\n",
    "                                      \"B01003_001E\": \"Total Population\",\n",
    "                                      \"B02001_002E\": \"Population: White\",\n",
    "                                      \"B02001_003E\": \"Population: Black\",\n",
    "                                      \"B15003_017E\": \"Education: High School\",\n",
    "                                      \"B15003_022E\": \"Education: Bachelors\",\n",
    "                                      \"NAME\": \"State\", \"state\": \"State Number\"})\n",
    "\n",
    "# Calculate & add in Poverty Rate (Poverty Count / Population)\n",
    "census_df[\"Poverty Rate\"] = 100 * \\\n",
    "    census_df[\"Poverty Count\"].astype(\n",
    "        int) / census_df[\"Total Population\"].astype(int)\n",
    "\n",
    "# Calculate & add in Unemployment Rate (Unemployment Count / Labor Force)\n",
    "census_df[\"Unemployment Rate\"] = 100 * \\\n",
    "    census_df[\"Unemployment Count\"].astype(\n",
    "        int) / census_df[\"Labor Force Size\"].astype(int)\n",
    "\n",
    "# Calculate and add share of population with at least a high school diploma.\n",
    "census_df['Share of Population with HS Diploma'] = 1 - \\\n",
    "    census_df['Education: High School'].astype(\n",
    "        int) / census_df['Total Population'].astype(int)\n",
    "\n",
    "census_df = census_df[[\"State\", \"Total Population\", \"Median Household Income\", \"Per Capita Income\", \"Unemployment Rate\",\n",
    "                         \"Poverty Rate\", \"Share of Population with HS Diploma\"]]\n",
    "\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GINI index for 2009.\n",
    "gini2017_csv = pd.read_csv('Resources/GINI2017.csv')\n",
    "\n",
    "# Merge csv with census DF on State.\n",
    "census_df = pd.merge(census_df, gini2017_csv, how='left', on='State')\n",
    "\n",
    "# Merge Hate Crime DF with census DF.\n",
    "clean_hate_crime_2017_df = pd.merge(census_df, state_hatecrime_2017_df, how=\"left\", on=\"State\")\n",
    "#clean_hate_crime_2017_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values for states w/ no data (reported hate crimes) with 0.\n",
    "# Hawaii is now the only stat w/ no data (reported hate crimes).\n",
    "values = {'Hate Crimes Committed': 0}\n",
    "clean_hate_crime_2017_df = clean_hate_crime_2017_df.fillna(value=values)\n",
    "#clean_hate_crime_2017_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Puerto Rico from DF.\n",
    "clean_hate_crime_2017_df = clean_hate_crime_2017_df.drop([51])\n",
    "#clean_hate_crime_2017_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_hate_crime_2017_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4498935fe5af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mclean_hate_crime_2017_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hate Crimes per 100,000'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     clean_hate_crime_2017_df['Hate Crimes Committed'].astype(\n\u001b[1;32m---> 11\u001b[1;33m         int) / clean_hate_crime_2017_df['Total Population'].astype(int)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Sort descending on Hate Crime Rate.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_hate_crime_2017_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate 2017 Hate Crime Rate per 100,000 total population.\n",
    "# https://oag.ca.gov/sites/all/files/agweb/pdfs/cjsc/prof10/formulas.pdf\n",
    "# Note:\t Calculating rates for geographies of less than 100,000 will generate an inflated rate\n",
    "# when compared to geographies with populations of 100,000 or more; therefore,\n",
    "# rates are not calculated for geographies with populations of less than 100,000.\n",
    "\n",
    "# Check Washington, D.C. on calculation, or if this is a result of massive increase in reported hate crimes!!!!!!\n",
    "\n",
    "clean_hate_crime_2017_df['Hate Crimes per 100,000'] = 100000 * \\\n",
    "    clean_hate_crime_2017_df['Hate Crimes Committed'].astype(\n",
    "        int) / clean_hate_crime_2017_df['Total Population'].astype(int)\n",
    "\n",
    "# Sort descending on Hate Crime Rate.\n",
    "clean_hate_crime_2017_df = clean_hate_crime_2017_df.sort_values(by='Hate Crimes per 100,000', ascending=False)\n",
    "clean_hate_crime_2017_df = clean_hate_crime_2017_df.reset_index(drop=True)\n",
    "clean_hate_crime_2017_df\n",
    "\n",
    "clean_hate_crime_2017_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For BIAS_DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all values in the BIAS_DESC column.\n",
    "\n",
    "#reduced_hate_crime_df['BIAS_DESC'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DF for type of hate crime.\n",
    "\n",
    "#bias_type_df = pd.DataFrame(reduced_hate_crime_df['BIAS_DESC'].values.tolist(), columns=['Racially Motivated','Religiously Motivated', 'Sexually Motivated', 'Multiple',])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
